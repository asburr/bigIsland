{
    "type": "composite",
    "desc": "Hallelujah commands",
    "files": {
        "type": "composite",
        "desc": "FILES generates a process-file for a previously unprocessed file.\nA process-file has the name '.process_<filename>', and the format:\n{\n'uuid': str, # Unique ID for this process-file\n'path': str, # Full path to the file that is to be processed.\n'found': int, # Timestamp of when process-file was first created.\n'processing': int, # Timestamp set by FILE when sending path to a command, and also updated by command while processing.\n'processed': int # Timestamp when processing is done and complete.\n}.\nIn the configuration, a regex is used to associated a file with a feed. process-file is created in the same directory as the file being processed. Except, when the directory is read-only, in this case the configuration has a reference to a writable file system (see 'readonly') and that is where the process-file is created.\n Commands that consume the process-file shall also process the contents of the associated file that is found at the 'path'.  While the command is processing the contents of the file, it is periodically updating process-file with the current timesamp in the 'processing' field.\nPeriodically, FILES checks for hung processing. A hung process is when the 'processing' timestamp is more than 10 minutes old. It is assumed that hung processing shall not recover, and the process-file is recreated with a new UUID. If a hung processing should recover and resume processing the file, it will find a different UUID in the process-file, at which point it shall shutdown, stop processing the file, and make no changes to the process-file.\nPeriodically FILES checks for processing that has finished. Finished is when the 'processed' field is not zero. FILES has three choices at the finish:\n1/ When read-only file sytems, the process-file continues to exist until something else removes the actual file; 2/ When archiving, the processed file is moved to the archive, and the process-file is removed; 3/ Otherwise, the processed file and process-file are erased.",
        "eg": "{p: { p:<path> d:1 ro:<path> fs:[ { f:<fileo> re:*\\.csv } ] a:{ p:<path> dateSubDir:day purge:60 diskUsage:70 onError:<path> } } }",
        "root": {
            "type": "str", "key": true,
            "desc": "Root name for the feed of file names."
        }, "path": {
            "type": "composite",
            "desc": "path to be processed",
            "path": {
                 "type": "path", "default": ".",
                 "desc": "Path to load files from. Default is current directory."
            }, "order": {
                 "type": "choice", "default": "mnewest",
                 "desc": "Order of file name retrieval",
                 "mnewest": {
                     "type": "bool", "default": true,
                     "desc": "Newest modified file first"
                 }
            }, "depth": {
                "type": "int", "default": 0,
                "desc": "Specifies the depth of subdir searching under input dir. Values are: no subdir(0); unlimited depth(-1); or, a depth(N)"
            }, "readonly": {
                "type": "path", "default": null,
                "desc": "(readonly) is used when path (path) is read only. Touch-files are created to track prior processed files. (readonly) is a path on a writable file system where the touch-files can be created and deleted"
            }, "feeds": {
                "type": "listComposites",
                "feed": {
                    "type": "feed",
                    "desc": "Label the feed of file names. Use format to include root, {root}.name."
                },
                "regex": {
                    "type": "regex",
                    "desc": "Regex string. For a single feed use '.*' to match all files"
                }
            }
        },
        "archive": {
            "type": "composite", "default": null,
            "desc": "Archive processed files",
            "path": {
                "type": "path",
                "desc": "Successful loading of the file is moved to this location"
            }, "dateSubDir": {
                "type": "str", "default": "day",
                "choice": ["day", "month", "year", "none"],
                "desc": "Archive subdir by date of <path>/<dateSubDir>/[<fileSubDir>/]<file>"
            }, "purge": {
                "type": "int", "default": 432000,
                "desc": "Age of archive file in seconds after which archived file is deleted"
            }, "diskUsage": {
                "type": "int", "default": 70,
                "desc": "Percentage usage of the file system where the archive is located, at or above this usage triggers the oldest archived files to be deleted"
            }, "onError": {
                "type": "path", "default": null,
                "desc": "Failure to load whole or part of a file, this file is moved to this archive. Default is to delete the file that's in error when not readonly"
            }
        }
    },
    "loadf": {
        "type": "composite",
        "desc": "Loads data from a file",
        "eg": "{files: <feed> f:<feed> fstats:<feed> s:{ tshark: false csv: 1 file:{ f: <feed> oracleTable: <table> csvRowCount: 1 } }",
        "snapshot": {
            "type": "bool", "default": false,
            "desc": "True when file contains a complete snapshot. Internally, this adds the snapshot indication to the feed, indicating the start and end of each snapshot. From a user's perspective, a join is paused until the first snapshot is received, subsequent snapshots are received peacemeal ."
        }, "files": {
             "type": "feedRef",
             "desc": "Input feed containing the file-names that are loaded"
        }, "feed": {
             "type": "feed", "key": true,
             "desc": "Output feed containing the loaded data"
        }, "fstats": {
            "type": "feed",
            "desc": "Output Feed containing stats about the loaded data"
        }, "schema": {
            "type": "choice",
            "desc": "Define how to get the schema",
            "tshark": {
                "type": "bool", "default": true,
                "desc": "Process pcap files through tshark using -t json and use predefined procedures to clean up the tshark json."
            }, "csvHeaderRow": {
                "type": "int", "default": 0,
                "desc": "CSV row count for files containing a CSV header row at the row count. Rows prior to the header row, are ignored and not processed as data nor headers."
            }, "file": {
                "type": "composite",
                "desc": "Schema is found in a separate file",
                "feed": {
                     "type": "feedRef",
                     "desc": "Inputted file name(s) containing the schema"
                }, "filetype": {
                    "type": "choice",
                    "desc": "Type of schema in the file",
                    "oracleTable": {
                         "type": "str", "default": "",
                         "desc": "When not default, schema is an Oracle formatted schema, and look for the schema labeled with the table-name"
                    }, "csvRowCount": {
                         "type": "int", "default": 0,
                         "desc": "When gt 0, schema is csv formatted headers. This is the row count to the header row for this schema. Count starts at 1."
                    }
                }
            }
        }, "errors": {
            "type": "feed", "key": true,
            "default": "{feed}_errors",
            "desc": "Output feed containing filename and error, for files that failed to load."
        }, "cleanup": {
            "type": "composite", "default": null,
            "desc": "Instructions to cleanup the json that is outputted by parsing the data. These instructions are in addition to any automatic cleanup already performed by parsing",
            "valuelabels": {
                "type": "path", "default": null,
                "desc": "name of file with a list of <blocklabel>,<keylabel>,<replacelabel>."
            }, "ignore": {
                "type": "path", "default": null,
                "desc": "name of file with a list of <label> to ignore"
            }, "nest": {
                "type": "path", "default": null,
                "desc": "name of file with list of <label> to nest"
            }, "trunc": {
                "type": "path", "default": null,
                "desc": "name of file with list of <label>, <value> to be truncated beyond value"
            }
        }
    }, "logicalcondition.macro": {
        "type": "composite",
        "desc": "logical condition i.e. l:a o:eq r:{ f:b }",
        "l": {
            "type": "field",
            "desc": "left handside of the condition is a field name"
        }, "o": {
            "type": "str", "default": "eq", "desc": "Logical condition",
            "choice": ["eq", "ne", "gt", "lt", "gte", "lte"]
        }, "r": {
            "type": "choice", "desc": "right handside",
            "f": {
                "type": "field",
                "desc": "field name"
            }, "v": {
                "type": "any",
                "desc": "value"
            }
        }
    }, "logicalexpression.macro": {
        "type": "composite",
        "desc": "A nested expression, i.e. a eq 1 and b eq 2, l:{ c:{ l:a o:eq r:{ v:1 }} o:and c:{ c:{ l:b o:eq: r:{ v:2 }}}}",
        "l": {
            "type": "choice", "desc": "Left handside of the expression",
            "c": {
                "type": "logicalcondition.macro", "desc": "condition"
            }, "e": {
                "type": "logicalexpression.macro", "desc": "expression"
            }
        }, "o": {
            "type": "str", "default": "and", "desc": "logicial condition",
            "choice": ["and", "or"]
        }, "c": {
            "type": "logicalcondition.macro", "desc": "condition"
        }
    }, "fieldwhere.macro": {
        "type": "composite",
        "desc": "Identification of fields and, can identify particular value. i.e. {f:fred, w:<expression>}",
        "f": {
            "type": "field"
        }, "w": {
            "type": "logicalexpression.macro", "default": "{}",
            "desc": "Default is the empty expression; all values."
        }
    }, "fieldpartition.macro": {
        "type": "composite",
        "desc": "Identification of fields and, may identify range of values. i.e. {f:fred, t:day}",
        "f": {
            "type": "field"
        }, "t": {
            "type": "str", "default": "day",
            "choice": ["month", "day", "hour"]
        }
    }, "loads": {
        "type": "composite",
        "desc": "Load a sum of field(s) identified explicitly by name or, the default is sum all number fields. Sum can be partitioned by field(s) from the same feed. { f:data i:datai fs:[ { f:<fieldwhere> } ] p:[ { f:<fieldPartition> w:<logicalExpression> } ] }",
        "feed": {
            "type": "feed", "key": true
        }, "input": {
            "type": "feedRef", "desc": "The other feed"
        }, "fields": {
            "type": "listComposites",
            "field": {
                "type": "fieldwhere.macro"
            }
        }, "partition": {
            "type": "listComposites",
            "field": {
                "type": "fieldpartition.macro"
            }
        }, "where": {
            "type": "logicalexpression.macro", "default": null,
            "desc": "restrict sum or, default is to sum all records"
        }
    }, "loadc": {
        "type": "composite",
        "desc": "Count occurence of values in field(s)",
        "feed": {
            "type": "feed", "key": true
        }, "input": {
            "type": "feedRef", "desc": "The other feed"
        }, "count type": {
            "type": "choice",
            "all": {
                "type": "bool", "default": true, "desc": "count all fields"
            }, "fields": {
                "type": "listComposites", "desc": "Fields to count",
                "field": {
                    "type": "fieldwhere.macro"
                }
            }
        }, "partition": {
            "type": "listComposites", "default": null, "desc": "Partition count",
            "field": {
                "type": "fieldpartition.macro"
            }
        }
    }, "expand": {
        "type": "composite",
        "desc": "Add fields from values from within the same feed or from another feed. Two ways to join with the other feed. First is an explicit list of feed's fields. Second is by datatype, a list of datatypes identifies the feed's fields. { f:data j:{ fs:[ { f:<field> } ] } }",
        "feed": {
            "type": "feed", "key": true
        }, "input": {
            "type": "feedRef", "desc": "The feed to expand"
        }, "join": {
            "type": "composite", "default": null,
            "desc": "Join",
            "fields": {
                "type": "listComposites", "default": null,
                "desc": "List of feeds to join on.",
                "field": {
                    "type": "field"
                }
            }, "datatype": {
                "type": "str", "default": null,
                "desc": "Datatype with which to identify the field to join on. Alert when datatype is first obsolete in the feed. All fields with the datatype are expanded."
            }, "with": {
                "type": "feedRef", "default": null,
                "desc": "Name of the other feed to join with, default is to join with itself"
            }, "on": {
                "type": "listComposites", "default": null,
                "desc": "List of fields to join on. If datatype, then one field expected in o.f. Otherwise for named fields, then o.f must match the number of names in fs.f",
                "field": {
                    "type": "field"
                }
            }
        }, "add": {
            "type": "composite", "default": null,
            "add": {
                "type": "str",
                "desc": "{typefield}.name or fred.name. Where name is the new field"
            }, "fields": {
                "type": "listComposites", "default": null,
                "desc": "List of fields.",
                "field": {
                    "type": "field"
                }
            }, "fmt": {
                "type": "fmt",
                "desc": "{} style string format for new field. [] use python array slicing i.e. {}[1:3]. len({}) use python len to get the length of field."
            }
        }
    }, "alert": {
        "type": "composite",
        "desc": "Email sent to alert of system issues.",
        "feed": {
            "type": "feed", "default": "{input}_alert", "key": true,
            "desc": "Name for the feed of alerts"
        }, "input": {
            "type": "feedRef", "default": null,
            "desc": "Flow of events that trigger the alert or, default is all errors"
        }, "sumtype": {
            "type": "str", "default": "SUM", "choice": ["SUMPARTITIONED", "SUM"],
            "desc": "SUMPARTITIONED, one alert for each event; SUM, one alert for all events."
        }, "email": {
            "type": "email",
            "desc": "Email a breakdown of files failed to load"
        }, "period": {
            "type": "int", "default": 86400,
            "desc": "This is the period in second in between the email alerts."
        }
    }, "_usage_": {
        "type": "bool",
        "desc": "Internal request for host usage from a summit Hallelu."
    }, "_usage_response_": {
        "type": "composite",
        "desc": "Host usage.",
        "host": {
            "type": "str",
            "desc": "Name of the host"
        }, "cpuUsage": {
            "type": "int",
            "desc": "Percentage (0-100) usage"
        }, "diskUsage": {
            "type": "int",
            "desc": "Percentage (0-100) usage"
        }, "memoryUsage": {
            "type": "int",
            "desc": "Percentage (0-100) usage"
        }, "timestamp": {
            "type": "str",
            "desc": "msdatetime.strftime() when usage was take"
        }
    }, "_request_cancel_": {
        "type": "composite",
        "desc": "Cancel the request. Sent by the client api when the client cancels the request and further received content for this request is being ignored by the api.",
        "requestId": {
            "type": "int",
            "desc": "The request's id"
        }
    }, "_sheetsReq_": {
        "type": "composite",
        "desc": "Gets uuid and filename for all of the sheets"
    }, "_sheetsCfm_": {
        "type": "composite",
        "desc": "list of sheets",
        "sheets": {
            "type": "listComposites", "desc": "List of sheets",
            "uuid": {
                "type": "str",
                "desc": "sheet UUID"
            },
            "filename": {
                "type": "str",
                "desc": "The file name that holds the sheet"
            }
        }
    }, "_sheetReq_": {
        "type": "composite",
        "desc": "Get a sheet",
        "uuid": {
            "type": "str",
            "desc": "sheet UUID"
        }
    }, "_sheetCfm_": {
        "type": "composite",
        "desc": "Copy of commands in the sheet",
        "uuid": {
            "type": "str",
            "desc": "cmd UUID"
        },
        "version": {
            "type": "str",
            "desc": "cmd version"
        },
        "cmd": {
            "type": "any",
            "desc": "cmd"
        }
    }, "_cmdReq_": {
        "type": "composite",
        "desc": "Update a cmd",
        "sheetUuid": {
            "type": "str",
            "desc": "sheet UUID"
        },
        "cmdUuid": {
            "type": "str",
            "desc": "cmd UUID"
        },
        "version": {
            "type": "str",
            "desc": "Version of the command prior to the update"
        },
        "cmd": {
            "type": "any",
            "desc": "The updated command"
        }
    }, "_cmdCfm_": {
        "type": "composite",
        "desc": "_cmdCfm_",
        "status": {
            "type": "str",
            "choice": ["complete", "wrong version"],
            "desc": "Update status."
        }
    }, "_streamReq_": {
        "type": "composite",
        "desc": "_streamReq_ is sent by the Receiver to the Hallelu to start a stream of data from the Hallelu's Jah. Same Message is forwarded to each Jah by Hallelu, to start stream processing which runs contineously building dataCfm messages so they are ready to be transmitted to the Receiver as soon as asked.",
        "streamUuid": {
            "type": "str",
            "desc": "stream UUID"
        }, "feed": {
            "type": "feedRef", "default": null,
            "desc": "Name a feed to stream"
        }, "burst": {
            "type": "int", "default": 1,
            "desc": "Streams are synchronized, burst is the maximum number of documents to send before waiting for a dataCfm."
        }, "streamType": {
            "type": "str", "default": "parallel",
            "choice": ["parallel", "parallelDistribution", "serial"],
            "desc": "Parallel is a Receiver for every Jah, and data is sent in parallel between Jahs and Receivers i.e. loadf _data_ to expand. ParallelDistribution has a Receiver for every Jah and must have distribution keys, the data is sent in parallel, and to not overload a Receiver it gets data from one Jah at a time i.e. loadf _data_ to expand with a join. Serial has one Receiver, there may be many Jah but one Jah at a time is sending _data_ to the Receiver i.e. loadf to an external Receiver like a GUI."
        }, "dataCols": {
            "type": "listComposites", "default": null,
            "desc": "The default is to stream all columns. Otherwise, this is a list of columns to be streamed.",
            "fieldname": {
                "type": "str",
                "desc": "field name"
            }
        }, "distribution": {
            "type": "listComposites", "default": null,
            "desc": "The default is no distribution. Distribution divides the stream using the key columns. The key may be a complex type such as dict and list. The key is hashed, last bits of the hash selects the destination.",
            "keyColumn": {
                "type": "str",
                "desc": "Column name"
            }, "wildchar_masks": {
                "type": "listComposites", "default": null,
                "desc": "Although Jah may have many masks per length of value, the stream command wants the first mask per length of value. this mask is applied before hashing",
                "wildchar": {
                    "type": "str",
                    "desc": "A single char that is the wildchar, i.e. X"
                }, "mask": {
                    "type": "str",
                    "desc": "Mask is 1 for hashable char and 0 for wildchar, i.e. 1101 matches value ABCD which is converted to the wildchar value of ABXD before hashing"
                }
            }
        }
    }, "_streamCfm_": {
        "type": "composite",
        "desc": "_streamCfm_ stop retransmit of _streamReq_. Contains the address of the Jah to be communicated with to get the stream's data. For parallel streams, there is a list of address for all jah, the receiver is expected to create a receiver for each Jah, and each receiver communicate with one of the Jah. For serial streams, there is one address entrypoint Jah. When sent from Jah, the list of jahs is not present.",
        "state": {
            "type": "str", "default": "success",
            "choice": ["success", "fail"],
            "desc": "State of stream"
        }, "jahs": {
            "type": "listComposites", "default": null,
            "desc": "Jahs",
            "ip": {
                "type": "str",
                "desc": "Ip address"
            }, "port": {
                "type": "int",
                "desc": "Port"
            }
        }
    }, "_streamCanReq_": {
        "type": "composite",
        "desc": "_streamCan_ is sent cancel the stream. By Receiver to Hallelu; By Hallelu to Jah.",
        "streamUuid": {
            "type": "str",
            "desc": "stream UUID"
        }
    }, "_streamCanCfm_": {
        "type": "composite",
        "desc": "_streamAck_ stops retransmit of _streamCan_",
        "streamUuid": {
            "type": "str",
            "desc": "stream UUID"
        }
    }, "_dataReq_": {
        "type": "composite",
        "desc": "_dataReq_ is sent to a Jah.",
        "streamUuid": {
            "type": "str",
            "desc": "stream UUID"
        }
    }, "_dataCfm_": {
        "type": "composite",
        "desc": "_data_ from Jah to Receiver.",
        "data": {
            "type": "any",
            "desc": "the data"
        }, "nxtJah": {
            "type": "composite",
            "desc": "Jah for next dataReq.",
            "ip": {
                "type": "str",
                "desc": "Ip address"
            }, "port": {
                "type": "int",
                "desc": "Port"
            }
        }
    }, "_dataTimeoutReq_": {
        "type": "composite",
        "desc": "_dataTimeoutReq_ is sent to a Hallelu when the dataReq timeouts out to Jah.",
        "streamUuid": {
            "type": "str",
            "desc": "stream UUID"
        }, "jah": {
            "type": "composite",
            "desc": "Jah that did not respond to dataReq.",
            "ip": {
                "type": "str",
                "desc": "Ip address"
            }, "port": {
                "type": "int",
                "desc": "Port"
            }
        }
    }, "_dataTimeoutCfm_": {
        "type": "composite",
        "desc": "_dataTimeoutCfm_ is sent back to Receiver, and indicates the replacement Jah.",
        "nxtJah": {
            "type": "composite",
            "desc": "The address of the replacement Jah, may be the same Jah for temporary problems.",
            "ip": {
                "type": "str",
                "desc": "Ip address"
            }, "port": {
                "type": "int",
                "desc": "Port"
            }
        }
    }
}